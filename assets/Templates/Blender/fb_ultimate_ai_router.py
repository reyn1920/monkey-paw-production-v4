#!/usr/bin/env python3
"""
ğŸš€ **ULTIMATE AI ROUTER** - Integrating ALL your AI services!

This script demonstrates how to use:
1. Abacus.AI Route LLM (smart routing)
2. HuggingFace Router (1000+ models)
3. Local Ollama (19 models)
4. Direct API access

Author: Your AI Assistant ğŸ¤–
Created: September 23, 2025
"""

import asyncio
import os
import time
from dataclasses import dataclass
from typing import List

import aiohttp
from openai import OpenAI


@dataclass
class AIResponse:
    provider: str
    model: str
    response: str
    cost: float
    latency: float
    quality_score: float


class UltimateAIRouter:
    """ğŸ¯ Route requests to the BEST AI service for each task!"""

    def __init__(self):
        # 1. Abacus.AI Route LLM Client
        self.abacus_client = OpenAI(
            base_url="https://api.abacus.ai/v1/route-llm",
            api_key="s2_1a17153cc0af4595b70c4bf92f64715e",
        )

        # 2. HuggingFace Router Client
        self.hf_client = OpenAI(
            base_url="https://router.huggingface.co/v1",
            api_key=os.environ.get("HF_TOKEN", "your_hf_token_here"),
        )

        # 3. Local Ollama Models (19 available!)
        self.local_models = [
            "phi3:3.8b",
            "llama3.2:1b",
            "deepseek-coder:6.7b",
            "qwen2.5-coder:7b",
            "llama3.2:3b",
            "codellama:7b",
            "llama2:13b",
            "mistral:7b",
            "llama2:7b",
            "llama3.2:latest",
            "llama3:8b",
            "llama3.1:latest",
            "gemma2:latest",
            "llama3:latest",
            "gemma3:latest",
            "llama2:latest",
            "llama3.1:8b",
            "llama3-chatqa:8b",
        ]

        # Task-to-provider mapping
        self.task_routing = {
            "coding": "local_coding",  # Use local DeepSeek-Coder for privacy
            "analysis": "abacus",  # Use Abacus for smart routing
            "creative": "huggingface",  # Use HF for model diversity
            "chat": "local_chat",  # Use local Llama for quick chat
            "research": "abacus",  # Use Abacus for complex reasoning
            "debugging": "local_coding",  # Keep debugging local
        }

    async def call_abacus(self, prompt: str, task_type: str = "general") -> AIResponse:
        """ğŸ¯ Call Abacus.AI with smart routing"""
        start_time = time.time()

        try:
            completion = self.abacus_client.chat.completions.create(
                model="auto",  # Let Abacus choose the best model
                messages=[{"role": "user", "content": prompt}],
                route_preferences={
                    "optimize_for": "cost" if task_type == "simple" else "quality",
                    "max_cost_per_token": 0.001,
                    "preferred_providers": ["openai", "anthropic", "google"],
                },
            )

            latency = time.time() - start_time
            response_text = completion.choices[0].message.content

            return AIResponse(
                provider="Abacus.AI",
                model="auto-routed",
                response=response_text,
                cost=0.001,  # Estimated
                latency=latency,
                quality_score=8.5,
            )

        except Exception as e:
            return AIResponse(
                provider="Abacus.AI",
                model="error",
                response=f"Error: {str(e)}",
                cost=0.0,
                latency=time.time() - start_time,
                quality_score=0.0,
            )

    async def call_huggingface(
        self, prompt: str, model: str = "microsoft/DialoGPT-medium"
    ) -> AIResponse:
        """ğŸ¤— Call HuggingFace Router"""
        start_time = time.time()

        try:
            completion = self.hf_client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0.7,
            )

            latency = time.time() - start_time
            response_text = completion.choices[0].message.content

            return AIResponse(
                provider="HuggingFace",
                model=model,
                response=response_text,
                cost=0.0005,  # Estimated
                latency=latency,
                quality_score=7.5,
            )

        except Exception as e:
            return AIResponse(
                provider="HuggingFace",
                model=model,
                response=f"Error: {str(e)}",
                cost=0.0,
                latency=time.time() - start_time,
                quality_score=0.0,
            )

    async def call_ollama(self, model: str, prompt: str) -> AIResponse:
        """ğŸ  Call local Ollama model"""
        start_time = time.time()

        try:
            async with aiohttp.ClientSession() as session:
                payload = {"model": model, "prompt": prompt, "stream": False}

                async with session.post(
                    "http://localhost:11434/api/generate", json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        response_text = data.get("response", "No response")

                        latency = time.time() - start_time

                        return AIResponse(
                            provider="Ollama Local",
                            model=model,
                            response=response_text,
                            cost=0.0,  # Free!
                            latency=latency,
                            quality_score=8.0 if "coder" in model else 7.0,
                        )
                    else:
                        raise Exception(f"HTTP {response.status}")

        except Exception as e:
            return AIResponse(
                provider="Ollama Local",
                model=model,
                response=f"Error: {str(e)}",
                cost=0.0,
                latency=time.time() - start_time,
                quality_score=0.0,
            )

    async def smart_route(self, prompt: str, task_type: str = "general") -> AIResponse:
        """ğŸ§  Intelligently route to the BEST service for the task"""

        if task_type == "coding":
            # Use local DeepSeek-Coder for privacy & speed
            return await self.call_ollama("deepseek-coder:6.7b", prompt)

        elif task_type == "analysis":
            # Use Abacus for smart model selection
            return await self.call_abacus(prompt, task_type)

        elif task_type == "creative":
            # Use HuggingFace for model diversity
            return await self.call_huggingface(prompt, "microsoft/DialoGPT-medium")

        elif task_type == "chat":
            # Use local Llama for quick responses
            return await self.call_ollama("llama3.1:8b", prompt)

        else:
            # Default to Abacus smart routing
            return await self.call_abacus(prompt, task_type)

    async def compare_all_services(self, prompt: str) -> List[AIResponse]:
        """ğŸ”¥ Get responses from ALL services and compare!"""

        tasks = [
            self.call_abacus(prompt),
            self.call_huggingface(prompt),
            self.call_ollama("llama3.1:8b", prompt),
            self.call_ollama("deepseek-coder:6.7b", prompt),
        ]

        responses = await asyncio.gather(*tasks)

        # Sort by quality score
        responses.sort(key=lambda x: x.quality_score, reverse=True)

        return responses

    def print_comparison(self, responses: List[AIResponse]):
        """ğŸ“Š Pretty print comparison results"""
        print("\n" + "=" * 80)
        print("ğŸš€ **AI SERVICE COMPARISON RESULTS** ğŸš€")
        print("=" * 80)

        for i, resp in enumerate(responses, 1):
            print(f"\n{i}. **{resp.provider}** ({resp.model})")
            print(f"   ğŸ’° Cost: ${resp.cost:.4f}")
            print(f"   âš¡ Latency: {resp.latency:.2f}s")
            print(f"   â­ Quality: {resp.quality_score}/10")
            print(f"   ğŸ“ Response: {resp.response[:150]}...")
            print("-" * 60)

        print("\nğŸ† **WINNER:** " + responses[0].provider)


async def main():
    """ğŸ¯ Demo the Ultimate AI Router!"""

    router = UltimateAIRouter()

    print("ğŸš€ **ULTIMATE AI ROUTER DEMO** ğŸš€")
    print("Integrating Abacus.AI + HuggingFace + 19 Local Ollama Models!")
    print("=" * 70)

    # Test prompts for different tasks
    test_cases = [
        {
            "prompt": "Write a Python function to calculate fibonacci numbers",
            "task_type": "coding",
        },
        {
            "prompt": "Explain quantum computing in simple terms",
            "task_type": "analysis",
        },
        {
            "prompt": "Write a creative story about a robot learning to paint",
            "task_type": "creative",
        },
        {"prompt": "Hi! How are you today?", "task_type": "chat"},
    ]

    for test in test_cases:
        print(f"\nğŸ¯ **Task:** {test['task_type'].upper()}")
        print(f"ğŸ“ **Prompt:** {test['prompt']}")
        print("-" * 50)

        # 1. Smart routing
        print("ğŸ§  **Smart Routing Result:**")
        result = await router.smart_route(test["prompt"], test["task_type"])
        print(f"   Provider: {result.provider}")
        print(f"   Response: {result.response[:100]}...")

        # 2. Compare all services (for first test case)
        if test == test_cases[0]:
            print("\nğŸ”¥ **Comparing ALL Services:**")
            all_responses = await router.compare_all_services(test["prompt"])
            router.print_comparison(all_responses)

    print("\nâœ¨ **Your AI Arsenal Summary:**")
    print("ğŸ¯ Abacus.AI Route LLM: Smart routing & cost optimization")
    print("ğŸ¤— HuggingFace Router: 1000+ model access")
    print("ğŸ  Local Ollama: 19 models for privacy & speed")
    print("ğŸ’» VS Code Extensions: 11 AI tools integrated")
    print("\nğŸŠ **Total AI Power: LEGENDARY!** ğŸŠ")


if __name__ == "__main__":
    asyncio.run(main())
